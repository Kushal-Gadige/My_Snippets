{
	"DataScienceImports": {
		"prefix": "dim1",
		"body": [
		  "# Imports",
		  "import os",
		  "import numpy                   as np",
		  "import pandas                  as pd ",
		  "import matplotlib.pyplot       as plt",
		  "import seaborn                 as sns",
		  "import warnings",
		  "warnings.filterwarnings('ignore')",
		  "",
		  "#importing plotly and cufflinks in offline mode",
		  "import cufflinks as cf",
		  "import plotly.offline",
		  "cf.go_offline()",
		  "cf.set_config_file(offline=False, world_readable=True)",
		  "",
		  "import plotly ",
		  "import plotly.express as px",
		  "import plotly.graph_objs as go",
		  "import plotly.offline as py",
		  "from plotly.offline import iplot",
		  "from plotly.subplots import make_subplots",
		  "import plotly.figure_factory as ff",
		  "",
		  "# Stats",
		  "from tqdm.notebook             import tqdm",
		  "import scipy.stats             as stats",
		  "from scipy.stats               import ttest_ind",
		  "",
		  "# Preprocessing and Decomposition",
		  "from sklearn                   import set_config",
		  "from sklearn.compose           import ColumnTransformer, make_column_transformer, make_column_selector",
		  "from sklearn.preprocessing     import binarize, Binarizer, StandardScaler, MinMaxScaler, RobustScaler",
		  "from sklearn.preprocessing     import OneHotEncoder, LabelEncoder, OrdinalEncoder",
		  "from sklearn.impute            import SimpleImputer, KNNImputer",
		  "from sklearn.decomposition     import TruncatedSVD, PCA",
		  "",
		  "# Train test split",
		  "from sklearn.model_selection   import train_test_split",
		  "",
		  "# Models",
		  "from sklearn.pipeline          import Pipeline",
		  "from sklearn.pipeline          import make_pipeline",
		  "from sklearn.ensemble          import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, StackingClassifier",
		  "from sklearn.ensemble          import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, BaggingRegressor, StackingRegressor",
		  "from sklearn.tree              import DecisionTreeClassifier, DecisionTreeRegressor",
		  "from sklearn.linear_model      import LogisticRegressionCV, LogisticRegression, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier, Perceptron",
		  "from sklearn.linear_model      import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron",
		  "",
		  "",
		  "from sklearn.svm               import SVC, LinearSVC, NuSVC, SVR, NuSVR, LinearSVR",
		  "from sklearn.neighbors         import KNeighborsClassifier, NearestCentroid",
		  "from sklearn.neighbors         import KNeighborsRegressor, NearestCentroid",
		  "from sklearn.naive_bayes       import GaussianNB, BernoulliNB, MultinomialNB",
		  "from sklearn.cluster           import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering",
		  "from sklearn.neural_network    import MLPClassifier, MLPRegressor",
		  "from xgboost                   import XGBClassifier, XGBRegressor",
		  "from lightgbm                  import LGBMClassifier, LGBMRegressor",
		  "from catboost                  import CatBoostClassifier, CatBoostRegressor",
		  "",
		  "# Cross Validation & Hyperparameter tuning",
		  "from sklearn.model_selection   import cross_val_score, KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold",
		  "from sklearn.model_selection   import GridSearchCV, RandomizedSearchCV ",
		  "",
		  "# mertics",
		  "from sklearn.metrics           import mean_squared_error, r2_score, accuracy_score",
		  "from sklearn.metrics           import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score",
		  "from sklearn.metrics           import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, plot_precision_recall_curve"
		],
		"description": "DataScienceImports"
	},
	"ReadDisplayCSV": {
		"prefix": "dfread2",
		"body": [
		  "df = pd.read_csv('${1:iris.csv}')",
		  "df.head()"
		],
		"description": "ReadDisplayCSV"
	},
	"EDA": {
		"prefix": "deda3",
		"body": [
		  "# EDA${1:}",
		  "df.info()${1:}",
		  "df.describe()",
		  "# df.Categorical.describe()${1:}",
		  "### Missing values${1:}",
		  "# check for missing values",
		  "# df.isnull().sum().sort_values(ascending=False).to_frame(name='Missing')",
		  "",
		  "def missing_values(df):",
		  "    total = df.isnull().sum().sort_values(ascending=False)",
		  "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)",
		  "    df_miss = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])",
		  "    return df_miss",
		  "missing_values(df)${1:}",
		  "### Duplicated Values${1:}",
		  "df.duplicated().sum()${1:}",
		  "### Unique values${1:}",
		  "df.nunique()${1:}",
		  "## Categorical & Numerical Features${1:}",
		  "df.columns${1:}",
		  "df.drop(['target'],axis=1).select_dtypes('number').nunique()${1:}",
		  "df.drop(['col'],axis=1).select_dtypes('object').nunique()${1:}",
		  "df.drop(['HeartDisease'],axis=1).select_dtypes('number').columns${1:}",
		  "df.drop(['col'],axis=1).select_dtypes('object').columns${1:}",
		  "low_cardinality_cols = [cname for cname in df.columns if df[cname].nunique() < 10 and df[cname].dtype == \"object\"]",
		  "low_cardinality_cols${1:}",
		  "numerical_cols = [cname for cname in df.columns if df[cname].dtype in ['int64', 'float64']]",
		  "numerical_cols${1:}",
		  "final_categorical_cols = low_cardinality_cols + numerical_cols${1:}",
		  "df_eda = df[final_categorical_cols].copy()${1:}",
		  "num = list()",
		  "cat = list()${1:}",
		  "print(f'Numerical: {num}' )",
		  "print(f'Categorical: {cat}' )${1:}",
		  "### Skew (Outliers)${1:}",
		  "print(\"Skew(Outliers): ___________________________\")",
		  "print(df.skew())",
		  "print(\"-----------------------------------------------\")",
		  "print(\"Kurtosis: __________________________\")",
		  "print(df.kurtosis())${1:}",
		  "### Correaltion${1:}",
		  "corr_df = pd.get_dummies(df, columns=cat, drop_first=True).corr()${1:}",
		  "corr_df['Churn_Yes'].sort_values().iloc[1:-1]${1:}",
		  "mask = np.triu(corr_df.values, k=1)",
		  "plt.figure(figsize=(12,10))",
		  "sns.heatmap(",
		  "    corr_df, annot=True, mask=mask, cmap='seismic',",
		  "    center=0, vmin=-1, vmax=1, fmt='.2f'",
		  ");${1:}",
		  "plt.figure(figsize=(10,4),dpi=200)",
		  "sns.barplot(x=corr_df['Churn_Yes'].sort_values().iloc[1:-1].index,y=corr_df['Churn_Yes'].sort_values().iloc[1:-1].values)",
		  "plt.title('Feature Correlation to Yes Churn')",
		  "plt.xticks(rotation=90);${1:}",
		  "## Target Variable${1:}",
		  "df['HeartDisease'].value_counts(normalize=True)*100, df['HeartDisease'].value_counts()${1:}",
		  "# Target analysis for classification",
		  "sns.countplot(df['HeartDisease'])",
		  "# df['Target'].iplot(kind='hist')",
		  "",
		  "# Target analysis for regression",
		  "# sns.displot(df['traget'])${1:}",
		  "## Numerical Features",
		  "### Univariate Analysis${1:}",
		  "df[num].describe()${1:}",
		  "# skew not greater than .75",
		  "df[num].skew()${1:}",
		  "df[num].iplot(kind='hist', subplots=True, bins=50);${1:}",
		  "# Distribution Plot",
		  "# for i in num:",
		  "#     sns.displot(x=i, data=df, col='HeartDisease')",
		  "",
		  "# or",
		  "",
		  "# Distribution Plot",
		  "plt.figure(figsize=(20,15))",
		  "for i in range(len(num)):",
		  "    ax = plt.subplot(3, 3, i+1)",
		  "    sns.histplot(df[num[i]],kde=True, bins=50)",
		  "    # sns.kdeplot(df[num[i]], shade=True, hue=df['HeartDisease'])",
		  "    plt.title(num[i])",
		  "    plt.tight_layout()",
		  "",
		  "",
		  "# kde plot",
		  "# for i in num:",
		  "#     sns.kdeplot(df[i], color=\"r\", label=\"target\", shade=True)",
		  "#     plt.show()${1:}",
		  "# Box Plot",
		  "plt.figure(figsize=(14,10))",
		  "sns.boxplot(data=df[num])${1:}",
		  "### Bivariate Analysis${1:}",
		  "# Pair Plot",
		  "plt.figure(figsize=(10,10))",
		  "sns.pairplot(df, hue='HeartDisease', size=3);${1:}",
		  "# try:",
		  "#     for i in range(len(num)):",
		  "#         sns.jointplot(x=num[i], y=num[i+1], data=df, hue='HeartDisease')",
		  "# except:",
		  "#     print(\"End of the list\")",
		  "",
		  "# jg = sns.jointplot(data=df, x='Age', y='Cholesterol', kind='reg')",
		  "# jg.plot(sns.scatterplot, sns.histplot)${1:}",
		  "## Categorical Features${1:}",
		  "for i in cat:",
		  "    # sns.countplot(x=i, hue='HeartDisease', data=df)",
		  "    # plt.show()",
		  "",
		  "    fig = px.histogram(df, x=i, color=\"HeartDisease\", width=400, height=400)",
		  "    fig.show()",
		  "",
		  "    print(round(df.groupby(i)['HeartDisease'].mean().sort_values(ascending=False)*100,2))",
		  "    print(\"_________________________________\")"
		],
		"description": "EDA"
	},
	"Preprocessing": {
		"prefix": "dpreprocess4",
		"body": [
		  "# Preprocessing",
		  "## Missing Values${1:}",
		  "# Drop null columns with 100% missing values",
		  "df_miss = missing_values(df)",
		  "null_columns = list(df_miss[df_miss['Percent']==1].index)",
		  "print(null_columns)",
		  "# df.drop(null_columns, axis=1, inplace=True)",
		  "",
		  "single_value_columns = df.nunique()[df.nunique() == 1].index.tolist()",
		  "print(single_value_columns)",
		  "# df.drop(single_value_columns, axis=1, inplace=True)",
		  "",
		  "# REMOVE ROWS with missing values",
		  "# df.dropna(inplace=True)",
		  "",
		  "# impute missing values with MEAN or MEDIAN",
		  "# df.fillna(df.mean(), inplace=True)",
		  "df['col'] = df['col'].replace(np.nan, df['col'].mean())",
		  "",
		  "",
		  "# impute categorical missing values with most frequent value",
		  "# df['col'] = df['col'].fillna(df['col'].mode()[0])",
		  "df['col'] = df['col'].fillna['most_freq_cat']",
		  "",
		  "# impute categorical missing values with new category(when no.of missing is large)",
		  "df['col'] = df['col'].fillna('new_cat')",
		  "",
		  "# impute missing values using last observation carried forward",
		  "df['col'] = df['col'].fillna(method='ffill')",
		  "# df['col'] = df['col'].ffill()",
		  "",
		  "# impute missing values using datawig",
		  "def impute_missing_values(df):",
		  "    imputer = datawig.SimpleImputer(missing_values=np.nan, strategy='mean')",
		  "    imputer.fit(df)",
		  "    return pd.DataFrame(imputer.transform(df), columns=df.columns)",
		  "",
		  "# for time series using interpolation",
		  "df[\"col\"] = df[\"col\"].interpolate(method='linear', limit_direction='forward', axis=0)",
		  "",
		  "#impute missing values using random forest",
		  "def impute_missing_values_random_forest(df):",
		  "    data = df.values",
		  "    X = data[:, 0:4]",
		  "    y = data[:, 4]",
		  "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)",
		  "    rf = RandomForestRegressor(n_estimators=100, random_state=0)",
		  "    rf.fit(X_train, y_train)",
		  "    y_pred = rf.predict(X_test)",
		  "    errors = abs(y_pred - y_test)",
		  "    mae = round(np.mean(errors), 2)",
		  "    return mae",
		  "",
		  "# impute missing values using decision tree",
		  "DT_bmi_pipe = Pipeline(steps=[ ",
		  "                               ('scale',StandardScaler()),",
		  "                               ('lr',DecisionTreeRegressor(random_state=42))",
		  "                              ])",
		  "X = df[['age','gender','bmi']].copy()",
		  "X.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)",
		  "",
		  "Missing = X[X.bmi.isna()]",
		  "X = X[~X.bmi.isna()]",
		  "Y = X.pop('bmi')",
		  "DT_bmi_pipe.fit(X,Y)",
		  "predicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),index=Missing.index)",
		  "df.loc[Missing.index,'bmi'] = predicted_bmi",
		  "",
		  "",
		  "# impute using linear regression",
		  "data = data[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]]",
		  "",
		  "data[\"Sex\"] = [1 if x==\"male\" else 0 for x in data[\"Sex\"]]",
		  "",
		  "test_data = data[data[\"Age\"].isnull()]",
		  "data.dropna(inplace=True)",
		  "",
		  "y_train = data[\"Age\"]",
		  "X_train = data.drop(\"Age\", axis=1)",
		  "X_test = test_data.drop(\"Age\", axis=1)",
		  "",
		  "model = LinearRegression()",
		  "model.fit(X_train, y_train)",
		  "",
		  "y_pred = model.predict(X_test)${1:}",
		  "## Dummies${1:}",
		  "## Split(Train Test)${1:}",
		  "# train test split",
		  "X = df.drop('HeartDisease', axis=1)",
		  "y = df['HeartDisease']",
		  "",
		  "X_train, X_test, y_train, y_test = train_test_split(",
		  "    X, y, test_size=0.25, random_state=1",
		  ")${1:}",
		  "# get dummies",
		  "df = pd.get_dummies(df, columns=cat, drop_first=True)",
		  "df.head()${1:}",
		  "## Scaling${1:}",
		  "# scaler = RobustScaler()",
		  "# sclaer = MinMaxScaler()",
		  "scaler = StandardScaler()",
		  "X_train = scaler.fit_transform(X_train)",
		  "X_test = scaler.transform(X_test)${1:}",
		  "## OR${1:}",
		  "## Pipeline Method${1:}",
		  "# 1",
		  "# ohe= OneHotEncoder()",
		  "# ct= make_column_transformer((ohe,categorical),remainder='passthrough') ",
		  "",
		  "",
		  "# 2",
		  "# ohe= OneHotEncoder()",
		  "# s= StandardScaler()",
		  "# ct1= make_column_transformer((ohe,cat),(s,num))  ",
		  "",
		  "",
		  "# 3",
		  "# set_config(display=\"diagram\")",
		  "# ",
		  "# num_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())",
		  "# ",
		  "# cat_proc = make_pipeline(",
		  "#     SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),",
		  "#     OneHotEncoder(handle_unknown=\"ignore\"),",
		  "# )",
		  "# ",
		  "# preprocessor = make_column_transformer(",
		  "#     (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))",
		  "# )",
		  "# ",
		  "# clf = make_pipeline(preprocessor, LogisticRegression())",
		  "# clf",
		  "",
		  "",
		  "# 4",
		  "# another method using make_column_selector",
		  "# one_hot_encoder = make_column_transformer(",
		  "#     (",
		  "#         OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),",
		  "#         make_column_selector(dtype_include=\"category\"),",
		  "#     ),",
		  "#     remainder=\"passthrough\",",
		  "# )",
		  "",
		  "# hist_one_hot = make_pipeline(",
		  "#     one_hot_encoder, HistGradientBoostingRegressor(random_state=42)",
		  "",
		  "# 5",
		  "# another method using ColumeTransformer",
		  "# preprocessor = ColumnTransformer(",
		  "#     transformers=[",
		  "#         (\"num\", StandardScaler(), num),",
		  "#         (\"cat\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), cat),",
		  "#     ],",
		  "#     remainder=\"passthrough\",",
		  "# )",
		  "",
		  "",
		  "# 6  ---- May be use this",
		  "# another method using ColumnTransformer with Pipeline for categorical data",
		  "preprocessor = ColumnTransformer(",
		  "    transformers=[",
		  "        (\"num\", StandardScaler(), num),",
		  "        (\"cat\", Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"onehot\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))]), cat),",
		  "    ],",
		  "    remainder=\"passthrough\",",
		  ")"
		],
		"description": "Preprocessing"
	},
	"Model Training": {
		"prefix": "dfit5",
		"body": [
		  "# Model Fitting${1:}",
		  "refer =[(\"LogR\", LogisticRegression(max_iter=1000)),",
		  "        (\"SVC\", SVC()),('LSVC',LinearSVC()),('KNN',KNeighborsClassifier(n_neighbors=10)),",
		  "         (\"DTC\", DecisionTreeClassifier()),(\"GNB\", GaussianNB()),",
		  "        (\"SGDC\", SGDClassifier()),(\"Perc\", Perceptron()),(\"NC\",NearestCentroid()),",
		  "        (\"Ridge\", RidgeClassifier()),(\"NuSVC\", NuSVC()),(\"BNB\", BernoulliNB()),",
		  "         ('RF',RandomForestClassifier()),('ADA',AdaBoostClassifier()),",
		  "        ('GB',GradientBoostingClassifier()),('PAC',PassiveAggressiveClassifier()),",
		  "        ('MLP',MLPClassifier()),('XGB',XGBClassifier()),",
		  "        ('LGBM',LGBMClassifier()),('CatBoost',CatBoostClassifier())]",
		  "",
		  "refer =[(\"LinR\", LinearRegression()),",
		  "        (\"SVR\", SVR()),('LSVR',LinearSVR()),('KNN',KNeighborsRegressor(n_neighbors=10)),",
		  "        (\"DTR\", DecisionTreeRegressor()),(\"GNB\", GaussianNB()),",
		  "        (\"SGDC\", SGDRegressor()),(\"Perc\", Perceptron()),(\"NC\",NearestCentroid()),",
		  "        (\"Ridge\", Ridge()),(\"NuSVR\", NuSVR()),(\"BNB\", BernoulliNB()),",
		  "        ('RF',RandomForestRegressor()),('ADA',AdaBoostRegressor()),",
		  "        ('GB',GradientBoostingRegressor()),('PAC',PassiveAggressiveRegressor()),",
		  "        ('MLP',MLPRegressor()),('XGB',XGBRegressor()),",
		  "        ('LGBM',LGBMRegressor()),('CatBoost',CatBoostRegressor())]",
		  "        ",
		  "models = [",
		  "    (\"LogR\", LogisticRegression(max_iter=1000)),",
		  "    ('RF',RandomForestClassifier()),",
		  "    ('GB',GradientBoostingClassifier()),",
		  "    ('XGB',XGBClassifier(eval_metric='logloss'))",
		  "]",
		  "",
		  "results = []",
		  "",
		  "pipe = Pipeline(",
		  "    steps=[",
		  "        (\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression()),",
		  "    ],",
		  ")",
		  "",
		  "for name,model in models:",
		  "    # pipe = make_pipeline(ct1, model)",
		  "    model.fit(X_train, y_train)",
		  "    y_pred = model.predict(X_test)",
		  "    precision = precision_score(y_test, y_pred, average='macro')",
		  "    f1 = f1_score(y_test, y_pred, average='macro')",
		  "    results.append((name,precision,f1))",
		  "    ",
		  "sorted_results = sorted(results, key = lambda x: x[1], reverse=True)${1:}",
		  "df1 = pd.DataFrame(sorted_results, columns=['Model', 'Precision', 'F1'])",
		  "df1${1:}",
		  "# change index in df1 with Model column",
		  "df1.set_index('Model', inplace=True)",
		  "df1${1:}",
		  "sns.heatmap(",
		  "    data=df1, cmap='seismic', annot=True,",
		  "    vmin=0.8, vmax=1, center=0.85, fmt='.2f'",
		  ");${1:}",
		  "### Ensembling With Multiple Models${1:}",
		  "# create an ensemble with logistic regression and random forest",
		  "ensemble = VotingClassifier(",
		  "    estimators=[('lr', LogisticRegression(max_iter=1000)), ('rf', RandomForestClassifier())],",
		  "    voting='soft',",
		  ")"
		],
		"description": "Model Training"
	},
	"Cross Validation": {
		"prefix": "dcrossval6",
		"body": [
		  "# Cross Validation",
		  "[Scoring details for CV (Link)](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)${1:}",
		  "# implement cross validation",
		  "kfold = KFold(n_splits=10, random_state=7)",
		  "results = cross_val_score(pipe, X_train, y_train, cv=kfold)",
		  "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))",
		  "",
		  "# implement cross validation with scoring",
		  "scoring = {'precision': 'precision_macro', 'recall': 'recall_macro'}",
		  "results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=scoring)",
		  "print(\"10-fold cross validation average precision: %.3f\" % (results.mean()['precision']))",
		  "",
		  "# implement cross validation with stratified kfold",
		  "skfold = StratifiedKFold(n_splits=10, random_state=7)",
		  "results = cross_val_score(pipe, X_train, y_train, cv=kfold)",
		  "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))"
		],
		"description": "Cross Validation"
	},
	"GridSearch": {
		"prefix": "dgrid7",
		"body": [
		  "# Grid Search${1:}",
		  "param_grid = {'n_estimators': [10, 50, 100, 200, 500],",
		  "              'max_features': ['auto', 'sqrt', 'log2'],",
		  "              'max_depth': [2, 3, 4, 5, 6, 7, 8],",
		  "              'min_samples_split': [2, 3, 4, 5, 6, 7, 8],",
		  "              'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8],",
		  "              'bootstrap': [True, False],",
		  "              'criterion': ['gini', 'entropy']}",
		  "",
		  "",
		  "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='f1_macro')",
		  "grid_search.fit(X_train, y_train)${1:}",
		  "# print best parameters",
		  "print(\"Best parameters found on development set:\")",
		  "print(grid_search.best_params_)",
		  "print(\"\")",
		  "print(\"Grid scores on development set:\")",
		  "means = grid_search.cv_results_['mean_test_score']",
		  "stds = grid_search.cv_results_['std_test_score']",
		  "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):",
		  "    print(\"%0.3f (+/-%0.03f) for %r\"",
		  "          % (mean, std * 2, params))",
		  "print(\"\")${1:}",
		  "# print best estimator",
		  "print(\"Detailed classification report:\")",
		  "print(\"\")",
		  "print(\"The model is trained on the full development set.\")",
		  "print(\"The scores are computed on the full evaluation set.\")",
		  "print(\"\")",
		  "y_true, y_pred = y_test, grid_search.predict(X_test)",
		  "print(classification_report(y_true, y_pred))",
		  "print(\"\")"
		],
		"description": "GridSearch"
	},
	"PlotMetrics": {
		"prefix": "dplotmet8",
		"body": [
		  "# Plotting Metrics${1:}",
		  "",
		  "# plot learning curve",
		  "# plot_learning_curve(pipe, \"Learning Curve\", X_train, y_train, ylim=(0.7, 1.01), cv=kfold)${1:}",
		  "# plot validation curve",
		  "# plot_validation_curve(pipe, \"Validation Curve\", X_train, y_train, param_name=\"max_depth\", param_range=np.arange(1, 10), cv=kfold)${1:}",
		  "# plot roc curve",
		  "# plot_roc_curve(pipe, X_test, y_test)${1:}",
		  "# plot precision-recall curve",
		  "# plot_precision_recall_curve(pipe, X_test, y_test)${1:}",
		  "# plot confusion matrix",
		  "# plot_confusion_matrix(pipe, X_test, y_test)${1:}",
		  "# plot feature importance",
		  "# plot_feature_importances(pipe, X_train, y_train)"
		],
		"description": "PlotMetrics"
	},
	"TradeOff(Thresh)": {
		"prefix": "dthresh9",
		"body": [
		  "# Precision Recall Threshold${1:}",
		  "logreg = LogisticRegression(max_iter=1000)",
		  "logreg.fit(X_train, y_train)",
		  "y_pred = logreg.predict(X_test)",
		  "print(confusion_matrix(y_test, y_pred))${1:}",
		  "# TN = [0,0]    FP = [0,1]",
		  "# FN = [1,0]    TP = [1,1]",
		  "",
		  "# Precision = TP / (TP + FP)",
		  "# Recall = TP / (TP + FN)",
		  "# F1 = 2 * Precision * Recall / (Precision + Recall)",
		  "# Sensitivity = Recall = True Positive Rate",
		  "# Specificity = TN / (TN + FP)",
		  "# Accuracy = (TP + TN) / (TP + TN + FP + FN)",
		  "#  False Positive Rate = FP / (FP + TN)",
		  "",
		  "",
		  "for i in range(1,6):",
		  "    ",
		  "    cm1=0",
		  "    y_pred1 = model.predict_proba(X_test)[:,1]",
		  "    y_pred1 = y_pred1.reshape(-1,1)",
		  "    y_pred2 = binarize(y_pred1, i/10)",
		  "    y_pred2 = np.where(y_pred2 == 1, 1, 0)",
		  "    cm1 = confusion_matrix(y_test, y_pred2)",
		  "        ",
		  "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n\\n',cm1,'\\n\\n',",
		  "            'with',cm1[0,0]+cm1[1,1],'correct predictions, ', '\\n\\n', ",
		  "           ",
		  "            cm1[0,1],'Type I errors( False Positives), ','\\n\\n',",
		  "           ",
		  "            cm1[1,0],'Type II errors( False Negatives), ','\\n\\n',",
		  "           ",
		  "           'Accuracy score: ', (accuracy_score(y_test, y_pred2)), '\\n\\n',",
		  "           'F1 score: ', (f1_score(y_test, y_pred2)), '\\n\\n',",
		  "           'Sensitivity: ',cm1[1,1]/(float(cm1[1,1]+cm1[1,0])), '\\n\\n',        # TP / (TP + FN)",
		  "           ",
		  "           'Specificity: ',cm1[0,0]/(float(cm1[0,0]+cm1[0,1])),'\\n\\n',        # TN / (TN + FP)",
		  "          ",
		  "            '====================================================', '\\n\\n')"
		],
		"description": "TradeOff(Thresh)"
	},







































	"Tensorflow imports": {
		"prefix": "itf",
		"body": [
		  "import os",
		  "import PIL",
		  "import pathlib",
		  "import numpy as np",
		  "import matplotlib.pyplot as plt",
		  "",
		  "import tensorflow as tf",
		  "from tensorflow import keras",
		  "from tensorflow.keras import layers",
		  "from tensorflow.keras.models import Sequential"
		],
		"description": "TF imports"
	},
	"TF ImageDsFromDir": {
		"prefix": "itfds",
		"body": [
		  "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'",
		  "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)",
		  "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')",
		  "",
		  "train_dir = os.path.join(PATH, 'train')",
		  "val_dir = os.path.join(PATH, 'validation')",
		  "",
		  "batch_size = 32",
		  "img_height = 180",
		  "img_width = 180",
		  "",
		  "train_ds = tf.keras.utils.image_dataset_from_directory(",
		  "    train_dir,",
		  "    shuffle=True,",
		  "    # validation_split=0.2,",
		  "    # subset=\"training\",",
		  "    # seed=123,",
		  "    image_size=(img_height, img_width),",
		  "    batch_size=batch_size)",
		  "",
		  "val_ds = tf.keras.utils.image_dataset_from_directory(",
		  "    val_dir,",
		  "    shuffle=True,",
		  "    # validation_split=0.2,",
		  "    # subset=\"validation\",",
		  "    # seed=123,",
		  "    image_size=(img_height, img_width),",
		  "    batch_size=batch_size)",
		  "",
		  "val_batches = tf.data.experimental.cardinality(val_ds)",
		  "test_ds = val_ds.take(val_batches // 5)",
		  "val_ds = val_ds.skip(val_batches // 5)",
		  "",
		  "AUTOTUNE = tf.data.AUTOTUNE",
		  "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)",
		  "val_ds_ds = val_ds.prefetch(buffer_size=AUTOTUNE)",
		  "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)",
		  ""
		],
		"description": "TF ImageDsFromDir"
	  }
}
