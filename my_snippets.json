{
	"DataScienceImports": {
		"prefix": "dim",
		"body": [
		  "# Imports",
		  "import os",
		  "import numpy                   as np",
		  "import pandas                  as pd ",
		  "import matplotlib.pyplot       as plt",
		  "import seaborn                 as sns",
		  "import warnings",
		  "warnings.filterwarnings('ignore')",
		  "",
		  "# Stats",
		  "from tqdm.notebook             import tqdm",
		  "import scipy.stats             as stats",
		  "from scipy.stats               import ttest_ind",
		  "",
		  "# Preprocessing and Decomposition",
		  "from sklearn.preprocessing     import StandardScaler, MinMaxScaler",
		  "from sklearn.decomposition     import TruncatedSVD, PCA",
		  "",
		  "# Train test split",
		  "from sklearn.model_selection   import train_test_split",
		  "",
		  "# Models",
		  "from sklearn.pipeline          import Pipeline",
		  "from sklearn.ensemble          import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier",
		  "from sklearn.tree              import DecisionTreeClassifier",
		  "from sklearn.linear_model      import LinearRegression, LogisticRegressionCV, LogisticRegression, RidgeClassifier, SGDClassifier",
		  "from sklearn.svm               import SVC, LinearSVC, NuSVC",
		  "from sklearn.neighbors         import KNeighborsClassifier, NearestCentroid",
		  "from sklearn.naive_bayes       import GaussianNB, BernoulliNB, MultinomialNB",
		  "from sklearn.cluster           import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering",
		  "from sklearn.neural_network    import MLPClassifier, MLPRegressor",
		  "",
		  "",
		  "# Cross Validation & Hyperparameter tuning",
		  "from sklearn.model_selection   import cross_val_score, GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold",
		  "",
		  "# mertics",
		  "from sklearn.metrics           import mean_squared_error, r2_score, accuracy_score",
		  "from sklearn.metrics           import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score"
		],
		"description": "DataScienceImports"
	},
	"ReadDisplayCSV": {
		"prefix": "df",
		"body": [
		  "df = pd.read_csv('${1:iris.csv}')",
		  "df.head()"
		],
		"description": "ReadDisplayCSV"
	},
	"InspectData": {
		"prefix": "dim",
		"body": [
		  "print(df.info())",
		  "print(df.describe())",
		  "",
		  "# check for missing values",
		  "# df.isnull().sum().sort_values(ascending=False).to_frame(name='Missing')",
		  "",
		  "def missing_values(df):",
		  "    total = df.isnull().sum().sort_values(ascending=False)",
		  "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)",
		  "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])",
		  "# missing_values(df)",
		  "",
		  "# check for duplicate values",
		  "df.duplicated().sum()",
		  "",
		  "# check for unique values",
		  "df.nunique()",
		  "",
		  "# check for correlation",
		  "df.corr()",
		  "",
		  "# check for outliers",
		  "df.skew()"
		],
		"description": "InspectData"
	},
	"Tensorflow imports": {
		"prefix": "itf",
		"body": [
		  "import os",
		  "import PIL",
		  "import pathlib",
		  "import numpy as np",
		  "import matplotlib.pyplot as plt",
		  "",
		  "import tensorflow as tf",
		  "from tensorflow import keras",
		  "from tensorflow.keras import layers",
		  "from tensorflow.keras.models import Sequential"
		],
		"description": "TF imports"
	},
	"TF ImageDsFromDir": {
		"prefix": "itfds",
		"body": [
		  "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'",
		  "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)",
		  "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')",
		  "",
		  "train_dir = os.path.join(PATH, 'train')",
		  "val_dir = os.path.join(PATH, 'validation')",
		  "",
		  "batch_size = 32",
		  "img_height = 180",
		  "img_width = 180",
		  "",
		  "train_ds = tf.keras.utils.image_dataset_from_directory(",
		  "    train_dir,",
		  "    shuffle=True,",
		  "    # validation_split=0.2,",
		  "    # subset=\"training\",",
		  "    # seed=123,",
		  "    image_size=(img_height, img_width),",
		  "    batch_size=batch_size)",
		  "",
		  "val_ds = tf.keras.utils.image_dataset_from_directory(",
		  "    val_dir,",
		  "    shuffle=True,",
		  "    # validation_split=0.2,",
		  "    # subset=\"validation\",",
		  "    # seed=123,",
		  "    image_size=(img_height, img_width),",
		  "    batch_size=batch_size)",
		  "",
		  "val_batches = tf.data.experimental.cardinality(val_ds)",
		  "test_ds = val_ds.take(val_batches // 5)",
		  "val_ds = val_ds.skip(val_batches // 5)",
		  "",
		  "AUTOTUNE = tf.data.AUTOTUNE",
		  "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)",
		  "val_ds_ds = val_ds.prefetch(buffer_size=AUTOTUNE)",
		  "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)",
		  ""
		],
		"description": "TF ImageDsFromDir"
	  }
}